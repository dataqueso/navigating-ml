<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.5.0 --> <title>Level 2 Preparation | Navigating ML</title> <meta name="generator" content="Jekyll v3.8.5" /> <meta property="og:title" content="Level 2 Preparation" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Introduction to Simple and Convolutional Neural Networks" /> <meta property="og:description" content="Introduction to Simple and Convolutional Neural Networks" /> <link rel="canonical" href="http://localhost:4000/navigating-ml/level2_prep/" /> <meta property="og:url" content="http://localhost:4000/navigating-ml/level2_prep/" /> <meta property="og:site_name" content="Navigating ML" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2018-11-06T00:00:00-08:00" /> <meta name="twitter:card" content="summary" /> <meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"description":"Introduction to Simple and Convolutional Neural Networks","@type":"BlogPosting","url":"http://localhost:4000/navigating-ml/level2_prep/","headline":"Level 2 Preparation","dateModified":"2018-11-06T00:00:00-08:00","datePublished":"2018-11-06T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/navigating-ml/level2_prep/"},"@context":"http://schema.org"}</script> <!-- End Jekyll SEO tag --> <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"> --> <link rel="stylesheet" href="/navigating-ml/css/main.css"> <link rel="alternate" type="application/rss+xml" title="Navigating ML" href="http://localhost:4000/navigating-ml/feed.xml"> <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> </head> <body> <header class="site-header"> <nav class="navbar navbar-default"> <div class="container-fluid"> <!-- Brand and toggle get grouped for better mobile display --> <div class="navbar-header"> <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/navigating-ml/">Navigating ML</a> </div> <!-- Collect the nav links, forms, and other content for toggling --> <div class="collapse navbar-collapse " id="bs-example-navbar-collapse-1"> <ul class="nav navbar-nav navbar-right"> <li><a href="/navigating-ml/about/">About</a></li> <li><a href="/navigating-ml/contact/">Contact</a></li> <li class="dropdown"> <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Download <span class="caret"></span></a> <ul class="dropdown-menu"> <li><a target="_blank" href="https://github.com/rheartpython/navigating-ml">Project</a></li> <li><a href="https://github.com/rheartpython/navigating-ml/archive/master.zip">Download</a></li> <li role="separator" class="divider"></li> </ul> </li> </ul> </div><!-- /.navbar-collapse --> </div><!-- /.container-fluid --> </nav> </header> <div class="container"> <div class="wrapper"> <div class="row"> <div class="col-md-8"> <article class="post" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <h1 class="post-title" itemprop="name headline">Level 2 Preparation</h1> <p class="post-meta"></p> </header> <div class="post-content" itemprop="articleBody"> <h1 id="introduction-to-simple-and-convolutional-neural-networks">Introduction to Simple and Convolutional Neural Networks</h1> <h2 id="classification-concepts-and-simple-nns">Classification Concepts and Simple NNs</h2> <p>Read through this excellent first taste of Image Classification from this Stanford CS231n course or, alternatively, watch the video (<a href="http://cs231n.github.io/classification/" target="_blank">Lecture Notes</a> or <a href="https://www.youtube.com/watch?v=OoUX-nOEjG0&amp;index=3&amp;t=0s&amp;list=PLzUTmXVwsnXod6WNdg57Yc3zFx_f-RYsq" target="_blank">Video</a>).</p> <p>Remind yourself of the MLP from Level 1 Practice (<a href="https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch12/ch12.ipynb" target="_blank">Code</a>).</p> <p>Now, explain the difference between the following two classes, the first a Nearest Neighbor class and the second a simple neural network called a Multi-Layer Perceptron:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">NearestNeighbor</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span>
    <span class="c1"># the nearest neighbor classifier simply remembers all the training data
</span>    <span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">=</span> <span class="n">X</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span> <span class="o">=</span> <span class="n">y</span>

  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="s">""" X is N x D where each row is an example we wish to predict label for """</span>
    <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># lets make sure that the output type matches the input type
</span>    <span class="n">Ypred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># loop over all test rows
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
      <span class="c1"># find the nearest training image to the i'th test image
</span>      <span class="c1"># using the L1 distance (sum of absolute value differences)
</span>      <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Xtr</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">min_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span> <span class="c1"># get the index with smallest distance
</span>      <span class="n">Ypred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ytr</span><span class="p">[</span><span class="n">min_index</span><span class="p">]</span> <span class="c1"># predict the label of the nearest example
</span>
    <span class="k">return</span> <span class="n">Ypred</span>
</code></pre></div></div> <p align="right">Source: http://cs231n.github.io/classification/</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">sys</span>


<span class="k">class</span> <span class="nc">NeuralNetMLP</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="s">""" Feedforward neural network / Multi-layer perceptron classifier.
    Parameters
    ------------
    n_hidden : int (default: 30)
        Number of hidden units.
    l2 : float (default: 0.)
        Lambda value for L2-regularization.
        No regularization if l2=0. (default)
    epochs : int (default: 100)
        Number of passes over the training set.
    eta : float (default: 0.001)
        Learning rate.
    shuffle : bool (default: True)
        Shuffles training data every epoch if True to prevent circles.
    minibatch_size : int (default: 1)
        Number of training samples per minibatch.
    seed : int (default: None)
        Random seed for initalizing weights and shuffling.
    Attributes
    -----------
    eval_ : dict
      Dictionary collecting the cost, training accuracy,
      and validation accuracy for each epoch during training.
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_hidden</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                 <span class="n">l2</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">l2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="n">minibatch_size</span>

    <span class="k">def</span> <span class="nf">_onehot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
        <span class="s">"""Encode labels into one-hot representation
        Parameters
        ------------
        y : array, shape = [n_samples]
            Target values.
        Returns
        -----------
        onehot : array, shape = (n_samples, n_labels)
        """</span>
        <span class="n">onehot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
            <span class="n">onehot</span><span class="p">[</span><span class="n">val</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="k">return</span> <span class="n">onehot</span><span class="o">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="s">"""Compute logistic function (sigmoid)"""</span>
        <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="o">-</span><span class="mi">250</span><span class="p">,</span> <span class="mi">250</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Compute forward propagation step"""</span>

        <span class="c1"># step 1: net input of hidden layer
</span>        <span class="c1"># [n_samples, n_features] dot [n_features, n_hidden]
</span>        <span class="c1"># -&gt; [n_samples, n_hidden]
</span>        <span class="n">z_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_h</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span>

        <span class="c1"># step 2: activation of hidden layer
</span>        <span class="n">a_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">z_h</span><span class="p">)</span>

        <span class="c1"># step 3: net input of output layer
</span>        <span class="c1"># [n_samples, n_hidden] dot [n_hidden, n_classlabels]
</span>        <span class="c1"># -&gt; [n_samples, n_classlabels]
</span>
        <span class="n">z_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_out</span>

        <span class="c1"># step 4: activation output layer
</span>        <span class="n">a_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sigmoid</span><span class="p">(</span><span class="n">z_out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">z_h</span><span class="p">,</span> <span class="n">a_h</span><span class="p">,</span> <span class="n">z_out</span><span class="p">,</span> <span class="n">a_out</span>

    <span class="k">def</span> <span class="nf">_compute_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_enc</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="s">"""Compute cost function.
        Parameters
        ----------
        y_enc : array, shape = (n_samples, n_labels)
            one-hot encoded class labels.
        output : array, shape = [n_samples, n_output_units]
            Activation of the output layer (forward propagation)
        Returns
        ---------
        cost : float
            Regularized cost
        """</span>
        <span class="n">L2_term</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">*</span>
                   <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_h</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">)</span> <span class="o">+</span>
                    <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_out</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">)))</span>

        <span class="n">term1</span> <span class="o">=</span> <span class="o">-</span><span class="n">y_enc</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="n">term2</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">y_enc</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">term1</span> <span class="o">-</span> <span class="n">term2</span><span class="p">)</span> <span class="o">+</span> <span class="n">L2_term</span>

        <span class="c1"># If you are applying this cost function to other
</span>        <span class="c1"># datasets where activation
</span>        <span class="c1"># values maybe become more extreme (closer to zero or 1)
</span>        <span class="c1"># you may encounter "ZeroDivisionError"s due to numerical
</span>        <span class="c1"># instabilities in Python &amp; NumPy for the current implementation.
</span>        <span class="c1"># I.e., the code tries to evaluate log(0), which is undefined.
</span>        <span class="c1"># To address this issue, you could add a small constant to the
</span>        <span class="c1"># activation values that are passed to the log function.
</span>        <span class="c1">#
</span>        <span class="c1"># For example:
</span>        <span class="c1">#
</span>        <span class="c1"># term1 = -y_enc * (np.log(output + 1e-5))
</span>        <span class="c1"># term2 = (1. - y_enc) * np.log(1. - output + 1e-5)
</span>
        <span class="k">return</span> <span class="n">cost</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="s">"""Predict class labels
        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
            Input layer with original features.
        Returns:
        ----------
        y_pred : array, shape = [n_samples]
            Predicted class labels.
        """</span>
        <span class="n">z_h</span><span class="p">,</span> <span class="n">a_h</span><span class="p">,</span> <span class="n">z_out</span><span class="p">,</span> <span class="n">a_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">z_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">):</span>
        <span class="s">""" Learn weights from training data.
        Parameters
        -----------
        X_train : array, shape = [n_samples, n_features]
            Input layer with original features.
        y_train : array, shape = [n_samples]
            Target class labels.
        X_valid : array, shape = [n_samples, n_features]
            Sample features for validation during training
        y_valid : array, shape = [n_samples]
            Sample labels for validation during training
        Returns:
        ----------
        self
        """</span>
        <span class="n">n_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># number of class labels
</span>        <span class="n">n_features</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1">########################
</span>        <span class="c1"># Weight initialization
</span>        <span class="c1">########################
</span>
        <span class="c1"># weights for input -&gt; hidden
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                      <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">))</span>

        <span class="c1"># weights for hidden -&gt; output
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">b_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_output</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                        <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_output</span><span class="p">))</span>

        <span class="n">epoch_strlen</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">))</span>  <span class="c1"># for progress formatting
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">eval_</span> <span class="o">=</span> <span class="p">{</span><span class="s">'cost'</span><span class="p">:</span> <span class="p">[],</span> <span class="s">'train_acc'</span><span class="p">:</span> <span class="p">[],</span> <span class="s">'valid_acc'</span><span class="p">:</span> <span class="p">[]}</span>

        <span class="n">y_train_enc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_onehot</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">n_output</span><span class="p">)</span>

        <span class="c1"># iterate over training epochs
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>

            <span class="c1"># iterate over minibatches
</span>            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">+</span>
                                   <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">):</span>
                <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">]</span>

                <span class="c1"># forward propagation
</span>                <span class="n">z_h</span><span class="p">,</span> <span class="n">a_h</span><span class="p">,</span> <span class="n">z_out</span><span class="p">,</span> <span class="n">a_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span>

                <span class="c1">##################
</span>                <span class="c1"># Backpropagation
</span>                <span class="c1">##################
</span>
                <span class="c1"># [n_samples, n_classlabels]
</span>                <span class="n">sigma_out</span> <span class="o">=</span> <span class="n">a_out</span> <span class="o">-</span> <span class="n">y_train_enc</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>

                <span class="c1"># [n_samples, n_hidden]
</span>                <span class="n">sigmoid_derivative_h</span> <span class="o">=</span> <span class="n">a_h</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">a_h</span><span class="p">)</span>

                <span class="c1"># [n_samples, n_classlabels] dot [n_classlabels, n_hidden]
</span>                <span class="c1"># -&gt; [n_samples, n_hidden]
</span>                <span class="n">sigma_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sigma_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span>
                           <span class="n">sigmoid_derivative_h</span><span class="p">)</span>

                <span class="c1"># [n_features, n_samples] dot [n_samples, n_hidden]
</span>                <span class="c1"># -&gt; [n_features, n_hidden]
</span>                <span class="n">grad_w_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sigma_h</span><span class="p">)</span>
                <span class="n">grad_b_h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sigma_h</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># [n_hidden, n_samples] dot [n_samples, n_classlabels]
</span>                <span class="c1"># -&gt; [n_hidden, n_classlabels]
</span>                <span class="n">grad_w_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_h</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sigma_out</span><span class="p">)</span>
                <span class="n">grad_b_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">sigma_out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Regularization and weight updates
</span>                <span class="n">delta_w_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_w_h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">w_h</span><span class="p">)</span>
                <span class="n">delta_b_h</span> <span class="o">=</span> <span class="n">grad_b_h</span>  <span class="c1"># bias is not regularized
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">w_h</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta_w_h</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">b_h</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta_b_h</span>

                <span class="n">delta_w_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_w_out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">w_out</span><span class="p">)</span>
                <span class="n">delta_b_out</span> <span class="o">=</span> <span class="n">grad_b_out</span>  <span class="c1"># bias is not regularized
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">w_out</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta_w_out</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">b_out</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">delta_b_out</span>

            <span class="c1">#############
</span>            <span class="c1"># Evaluation
</span>            <span class="c1">#############
</span>
            <span class="c1"># Evaluation after each epoch during training
</span>            <span class="n">z_h</span><span class="p">,</span> <span class="n">a_h</span><span class="p">,</span> <span class="n">z_out</span><span class="p">,</span> <span class="n">a_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_cost</span><span class="p">(</span><span class="n">y_enc</span><span class="o">=</span><span class="n">y_train_enc</span><span class="p">,</span>
                                      <span class="n">output</span><span class="o">=</span><span class="n">a_out</span><span class="p">)</span>

            <span class="n">y_train_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
            <span class="n">y_valid_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>

            <span class="n">train_acc</span> <span class="o">=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">y_train_pred</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span>
                         <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">valid_acc</span> <span class="o">=</span> <span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_valid</span> <span class="o">==</span> <span class="n">y_valid_pred</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span> <span class="o">/</span>
                         <span class="n">X_valid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

            <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\r</span><span class="si">%0*</span><span class="s">d/</span><span class="si">%</span><span class="s">d | Cost: </span><span class="si">%.2</span><span class="s">f '</span>
                             <span class="s">'| Train/Valid Acc.: </span><span class="si">%.2</span><span class="s">f</span><span class="si">%%</span><span class="s">/</span><span class="si">%.2</span><span class="s">f</span><span class="si">%% </span><span class="s">'</span> <span class="o">%</span>
                             <span class="p">(</span><span class="n">epoch_strlen</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span>
                              <span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">valid_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">eval_</span><span class="p">[</span><span class="s">'cost'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_</span><span class="p">[</span><span class="s">'train_acc'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_</span><span class="p">[</span><span class="s">'valid_acc'</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">valid_acc</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></div> <p align="right">Source: https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch12/ch12.ipynb</p> <h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2> <blockquote> <p><em>ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.</em> â€“CS231n Course</p> </blockquote> <p>Read or watch the following Stanford CS231n Lecture for an in-depth introduction to convolutional neural networks (ConvNets/CNNs). At the end of this 1-hr investment of time, you will have a thorough conceptual and practical understanding of CNNs and a good foundation to set forth proper applications of this type of NN architecture to images and other types of inputs (yes, CNNs have been used for music, speech and more):</p> <ul> <li><a href="http://cs231n.github.io/convolutional-networks/" target="_blank">Lecture Notes</a> or <a href="https://www.youtube.com/watch?v=bNb2fEVKeEo&amp;index=6&amp;t=0s&amp;list=PLzUTmXVwsnXod6WNdg57Yc3zFx_f-RYsq" target="_blank">Video</a></li> </ul> <p>Extra reading:</p> <ul> <li><a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank">Convolutions - what are they</a></li> <li><a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank">CNNs</a></li> </ul> <p>Learn concepts around neural networks for image applications and be able to answer these questions:</p> <ol> <li>Should you use the test set for hyperparameter tuning? What is the purpose of the validation set?</li> <li>List the differences between a Convolutional (ConvNet/CNN) and regular Dense Neural Network (e.g Multilayer Perceptron)</li> <li>List and give examples of the tunable hyperparameters one can find in a ConvNet.</li> <li>How can transfer learning speed up training? If a network is already built, is it possible to modify it for a different number of output classes?</li> </ol> <p>Gain familiarity with the PyTorch deep learning framework as it will be used in the Practice section so we can escape the use of CPU-bound <code class="highlighter-rouge">numpy</code> tensors for everything:</p> <ol> <li>PyTorch tensor vs. numpy array (see the PyTorch <a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Docs</a>)</li> <li>Transfer Learning with a CNN (how it often happens as CNNs can be big and take time and power to train from scratch) <ul> <li>Understand what is going on in this code snippet (from <a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Docs</a>):<pre><code class="language-pythonn">model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, 2)
</code></pre></li> </ul> </li> </ol> </div> </article> <div class="row"> <div id="disqus_thread"></div> <script defer> (function() { var d = document, s = d.createElement('script'); s.src = '//navigating-ml.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> </div> <div class="row"> <ul class="pager"> <li><a class="next" href="/navigating-ml/level1_practice/">&laquo; Level 1 Practice</a></li> <li><a class="previous" href="/navigating-ml/level2_practice/">Level 2 Practice &raquo;</a></li> </ul> </div> </div> <div class="col-md-4 mt20"> <div class="post-img"> <img width="600" src="/navigating-ml/images/fifth.jpg" alt="Level 2 Preparation"> </div> <div class="mt10 recent"> <h2>Related</h2> <ul> <li> <p><a href="/navigating-ml/setup/">Data Science Setup</a></p> </li> <li> <p><a href="/navigating-ml/level1_prep/">Level 1 Preparation</a></p> </li> <li> <p><a href="/navigating-ml/level1_practice/">Level 1 Practice</a></p> </li> </ul> </div> <br> </div> </div> </div> </div> <footer> <div class="container"> <div class="row p20"> <div class="col-md-4 text-center mt25">All rights reserved by <a target="_blank" href="">micheleen harris</a></div> <div class="col-md-4 text-center mt25" > </div> <div class="col-md-4 text-center mt25"> <a target="_blank" href="http://twitter.com/@rheartpython"><li class="social twitter"><i class="fa fa-twitter-square"></i></li></a> <a target="_blank" href="http://github.com/rheartpython"><li class="social github"><i class="fa fa-github-square"></i></li></a> </div> </div> </div> </footer> <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.2/jquery.min.js"></script> <script src="/navigating-ml/js/bootstrap.min.js"></script> <!-- Google Analytics Tracking code --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-129282757-1', 'auto'); ga('send', 'pageview'); </script> </body> </html>
